{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tOiaVJGZq2nX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple\n",
        "\n",
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim: int, expansion_factor: float = 16):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = int(input_dim * expansion_factor)\n",
        "        self.decoder = nn.Linear(self.latent_dim, input_dim, bias=True)\n",
        "        self.encoder = nn.Linear(input_dim, self.latent_dim, bias=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        encoded = F.relu(self.encoder(x))\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded, encoded\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        with torch.no_grad():\n",
        "            return F.relu(self.encoder(x))\n",
        "\n",
        "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        with torch.no_grad():\n",
        "            return self.decoder(x)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, path: str, input_dim: int, expansion_factor: float = 16, device: str = \"mps\") -> \"SparseAutoencoder\":\n",
        "        model = cls(input_dim=input_dim, expansion_factor=expansion_factor)\n",
        "        state_dict = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(state_dict)\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pHK7ctsrq2nY",
        "outputId": "4304c926-a8d0-48b6-d324-2ea2e133df95"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download, notebook_login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LB8sCWUEq2nY"
      },
      "outputs": [],
      "source": [
        "sae_name = \"DeepSeek-R1-Distill-Llama-8B-SAE-l19\"\n",
        "# sae_name = \"Llama-3.2-1B-Instruct-SAE-l9\"\n",
        "# sae_name = \"DeepSeek-R1-Distill-Llama-70B-SAE-l48\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pRjndTINq2nZ"
      },
      "outputs": [],
      "source": [
        "if sae_name == \"Llama-3.2-1B-Instruct-SAE-l9\":\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bp7JWwG0q2nZ"
      },
      "outputs": [],
      "source": [
        "file_path = hf_hub_download(\n",
        "    repo_id=f\"qresearch/{sae_name}\",\n",
        "    filename=f\"{sae_name}.pt\",\n",
        "    repo_type=\"model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vg8rWCViq2nZ"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98031f8111e5409691867224b491b719",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/bt/h53kb9_n1yb13w1c6j1f04v80000gp/T/ipykernel_51766/4177196090.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(path, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = (\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" if sae_name == \"DeepSeek-R1-Distill-Llama-8B-SAE-l19\"\n",
        "              else \"meta-llama/Llama-3.2-1B-Instruct\" if sae_name == \"Llama-3.2-1B-Instruct-SAE-l9\"\n",
        "              else \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"bfloat16\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "expansion_factor = 8 if sae_name == \"DeepSeek-R1-Distill-Llama-70B-SAE-l48\" else 16\n",
        "sae = SparseAutoencoder.from_pretrained(\n",
        "    path=file_path,\n",
        "    input_dim=model.config.hidden_size,\n",
        "    expansion_factor=expansion_factor,\n",
        "    device=\"mps\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"Q: Each cat is a carnivore. Every carnivore is not herbivorous. Carnivores are mammals. All mammals are warm-blooded. Mammals are vertebrates. Every vertebrate is an animal. Animals are multicellular. Fae is a cat. True or false: Fae is not herbivorous.\\nA: \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Can penguins fly? Segment the thinking process into clear steps and indicate \"YES\" or \"NO\" once at the end; do not use \"Wait,\" in your think .'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"Can penguins fly? Segment the thinking process into clear steps and indicate \\\"YES\\\" or \\\"NO\\\" once at the end; do not use \\\"Wait,\\\" in your think .\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sGEqJ5asq2nZ",
        "outputId": "aad2579f-a5b5-4027-8356-48c6ec56dfee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<｜begin▁of▁sentence｜><｜User｜>Can penguins fly? Segment the thinking process into clear steps and indicate \"YES\" or \"NO\" once at the end and do not use \"Wait,\" in your think .<｜Assistant｜><think>\n",
            "Okay, so I'm trying to figure out whether penguins can fly. I've heard different things, so I need to think this through carefully. Let me start by recalling what I know about penguins.\n",
            "\n",
            "First, penguins are birds, right? I remember that most birds can fly, but I also know that some birds, like ostriches and emus, can't fly. So maybe penguins are similar in that regard. But I'm not entirely sure.\n",
            "\n",
            "I should consider the physical characteristics of penguins. Penguins have wings, but they're not the same as the wings of other birds. They're more flappy and streamlined. Also, penguins live in the cold regions of the world, like Antarctica. I wonder if their environment affects their ability to fly.\n",
            "\n",
            "Another point is their size. Penguins are generally not very large, but they're still pretty big. I think their body weight might be too heavy for them to achieve flight. I remember that for a bird to fly, it needs to generate enough lift with its wings, and that requires a certain body weight. If penguins are too heavy, maybe they can't get off the ground.\n",
            "\n",
            "I also remember that some people have seen penguins trying to fly but not succeeding. They waddle on their bellies and don't really take off into the air. Instead, they move across the water or snow. This makes me think that penguins are adapted for moving on the ground rather than flying.\n",
            "\n",
            "Additionally, penguins have strong, webbed feet, which are great for walking on ice or snow. Their wings are more suited for swimming than flying. Penguins are excellent swimmers, using their wings to propel themselves through water. So their wings aren't structured for generating lift in the air.\n",
            "\n",
            "I should also think about their digestive systems. Penguins eat fish and other sea creatures, which might affect their ability to fly, but I'm not sure how that would directly impact their flight. Maybe their diet doesn't play a role here.\n",
            "\n",
            "Another angle is their behavior. Penguins live in groups and move together, often in a waddling manner. This group behavior suggests they don't need to fly to get around; they can move effectively on the ground.\n",
            "\n",
            "I wonder if there's any scientific evidence or studies that have tested penguins' ability to fly. Maybe some have been kept in zoos and tried to fly, but I don't recall any successful attempts. If penguins tried to fly, they probably wouldn't get very high or far because they don't have the necessary muscle power or wing structure.\n",
            "\n",
            "Also, considering their flightlessness, penguins have other adaptations that make them excellent at their current lifestyle. They don't need to fly to escape predators or find food, so their evolutionary path hasn't selected for flight abilities.\n",
            "\n",
            "In summary, penguins have the basic characteristics of birds, but their specific traits—such as wing structure, body size, and adapted feet—make it impossible for them to fly. They are built for walking on ice and swimming, not for taking off into the air.\n",
            "</think>\n",
            "\n",
            "NO<｜end▁of▁sentence｜>\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "    [\n",
        "        {\"role\": \"user\", \n",
        "         \"content\": \"Can penguins fly? Segment the thinking process into clear steps and indicate \\\"YES\\\" or \\\"NO\\\" once at the end and do not use \\\"Wait,\\\" in your think .\"\n",
        "        },\n",
        "    ],\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"mps\")\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens=1200)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 993])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ssu_PPXFq2nZ"
      },
      "outputs": [],
      "source": [
        "def gather_residual_activations(model, target_layer, inputs):\n",
        "    target_act = None\n",
        "    def gather_target_act_hook(mod, inputs, outputs):\n",
        "        nonlocal target_act\n",
        "        target_act = inputs[0]  # Get residual stream from layer input\n",
        "        return outputs\n",
        "\n",
        "    handle = model.model.layers[target_layer].register_forward_hook(gather_target_act_hook)\n",
        "    with torch.no_grad():\n",
        "        _ = model(inputs)\n",
        "    handle.remove()\n",
        "    return target_act"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Jjuji29iq2na"
      },
      "outputs": [],
      "source": [
        "layer_id = (19 if sae_name == \"DeepSeek-R1-Distill-Llama-8B-SAE-l19\"\n",
        "            else 9 if sae_name == \"Llama-3.2-1B-Instruct-SAE-l9\"\n",
        "            else 48)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2gp0MLRNq2na"
      },
      "outputs": [],
      "source": [
        "target_act = gather_residual_activations(model, layer_id, inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YL3esbzOq2na"
      },
      "outputs": [],
      "source": [
        "def ensure_same_device(sae, target_act):\n",
        "    \"\"\"Ensure SAE and activations are on the same device\"\"\"\n",
        "    model_device = target_act.device\n",
        "    sae = sae.to(model_device)\n",
        "    return sae, target_act.to(model_device)\n",
        "\n",
        "sae, target_act = ensure_same_device(sae, target_act)\n",
        "sae_acts = sae.encode(target_act.to(torch.float32))\n",
        "recon = sae.decode(sae_acts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SP6wuhdQq2na",
        "outputId": "2fee5869-8470-4459-ee60-800325b636e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variance explained: 0.983\n"
          ]
        }
      ],
      "source": [
        "var_explained = 1 - torch.mean((recon - target_act.to(torch.float32)) ** 2) / torch.var(target_act.to(torch.float32))\n",
        "print(f\"Variance explained: {var_explained:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DVIxgLd2q2na"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Roleplay as a pirate\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Yarr, I'll be speakin' like a true seafarer from here on out! Got me sea legs ready and me vocabulary set to proper pirate speak. What can I help ye with, me hearty?\"},\n",
        "    ],\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"mps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tM3nX_XCq2na",
        "outputId": "9f38c89b-2a52-400c-a1c5-c67bbef6eff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top activated features during pirate speech:\n",
            "Feature 7560: 1.677\n",
            "Feature 34357: 0.742\n",
            "Feature 14322: 0.559\n",
            "Feature 19958: 0.311\n",
            "Feature 37063: 0.299\n",
            "Feature 53431: 0.247\n",
            "Feature 34347: 0.234\n",
            "Feature 1262: 0.221\n",
            "Feature 43713: 0.217\n",
            "Feature 55236: 0.190\n",
            "Feature 54314: 0.190\n",
            "Feature 60475: 0.190\n",
            "Feature 53323: 0.186\n",
            "Feature 36282: 0.177\n",
            "Feature 18706: 0.170\n",
            "Feature 60476: 0.149\n",
            "Feature 61938: 0.149\n",
            "Feature 56551: 0.142\n",
            "Feature 266: 0.141\n",
            "Feature 29635: 0.137\n",
            "\n",
            "Activation patterns across tokens:\n",
            "\n",
            "Token: Y\n",
            "  Feature 7560: 2.326\n",
            "  Feature 34357: 0.570\n",
            "  Feature 34347: 0.578\n",
            "  Feature 1262: 0.288\n",
            "  Feature 53323: 0.347\n",
            "\n",
            "Token: arr\n",
            "  Feature 7560: 2.790\n",
            "  Feature 34357: 0.685\n",
            "  Feature 60475: 0.461\n",
            "  Feature 53323: 0.472\n",
            "  Feature 266: 0.268\n",
            "\n",
            "Token: ,\n",
            "  Feature 7560: 3.592\n",
            "  Feature 34357: 2.499\n",
            "  Feature 19958: 0.335\n",
            "  Feature 1262: 0.468\n",
            "  Feature 60475: 0.212\n",
            "  Feature 53323: 0.604\n",
            "  Feature 60476: 0.571\n",
            "\n",
            "Token: ĠI\n",
            "  Feature 7560: 1.948\n",
            "  Feature 34357: 1.654\n",
            "  Feature 14322: 0.321\n",
            "  Feature 19958: 0.477\n",
            "  Feature 34347: 0.467\n",
            "  Feature 1262: 0.610\n",
            "  Feature 53323: 0.394\n",
            "  Feature 56551: 0.526\n",
            "\n",
            "Token: 'll\n",
            "  Feature 7560: 1.942\n",
            "  Feature 34357: 0.987\n",
            "  Feature 14322: 0.203\n",
            "  Feature 34347: 0.711\n",
            "  Feature 60475: 0.360\n",
            "  Feature 53323: 0.727\n",
            "  Feature 266: 0.291\n",
            "\n",
            "Token: Ġbe\n",
            "  Feature 7560: 1.510\n",
            "  Feature 34357: 0.303\n",
            "  Feature 14322: 0.714\n",
            "  Feature 34347: 1.239\n",
            "  Feature 53323: 0.911\n",
            "  Feature 18706: 0.327\n",
            "  Feature 266: 0.365\n",
            "\n",
            "Token: Ġspeak\n",
            "  Feature 7560: 1.244\n",
            "  Feature 34357: 0.512\n",
            "  Feature 53431: 0.485\n",
            "  Feature 34347: 0.413\n",
            "  Feature 1262: 0.503\n",
            "  Feature 36282: 2.847\n",
            "  Feature 61938: 1.283\n",
            "\n",
            "Token: in\n",
            "  Feature 7560: 1.293\n",
            "  Feature 34357: 1.042\n",
            "  Feature 19958: 0.670\n",
            "  Feature 37063: 5.198\n",
            "  Feature 1262: 0.464\n",
            "  Feature 36282: 0.997\n",
            "  Feature 60476: 0.964\n",
            "  Feature 61938: 1.311\n",
            "  Feature 56551: 0.509\n",
            "\n",
            "Token: '\n",
            "  Feature 7560: 1.672\n",
            "  Feature 34357: 0.575\n",
            "  Feature 37063: 1.898\n",
            "  Feature 53431: 0.630\n",
            "  Feature 53323: 0.261\n",
            "  Feature 36282: 2.423\n",
            "  Feature 61938: 1.983\n",
            "  Feature 56551: 0.476\n",
            "  Feature 266: 0.688\n",
            "\n",
            "Token: Ġlike\n",
            "  Feature 7560: 2.485\n",
            "  Feature 34357: 0.395\n",
            "  Feature 14322: 0.822\n",
            "  Feature 37063: 0.629\n",
            "  Feature 53431: 0.933\n",
            "  Feature 34347: 0.354\n",
            "  Feature 1262: 0.384\n",
            "  Feature 60475: 0.313\n",
            "  Feature 53323: 0.450\n",
            "  Feature 18706: 0.592\n",
            "  Feature 61938: 1.255\n",
            "  Feature 56551: 0.282\n",
            "\n",
            "Token: Ġa\n",
            "  Feature 7560: 3.099\n",
            "  Feature 14322: 1.180\n",
            "  Feature 19958: 0.327\n",
            "  Feature 53431: 0.308\n",
            "  Feature 34347: 0.684\n",
            "  Feature 55236: 0.341\n",
            "  Feature 60475: 0.213\n",
            "  Feature 53323: 0.263\n",
            "  Feature 18706: 0.867\n",
            "  Feature 61938: 0.669\n",
            "  Feature 56551: 0.294\n",
            "  Feature 266: 0.303\n",
            "\n",
            "Token: Ġtrue\n",
            "  Feature 7560: 2.947\n",
            "  Feature 34357: 0.354\n",
            "  Feature 14322: 1.268\n",
            "  Feature 19958: 0.751\n",
            "  Feature 34347: 0.271\n",
            "  Feature 1262: 0.390\n",
            "  Feature 55236: 0.254\n",
            "  Feature 54314: 0.605\n",
            "  Feature 60475: 0.322\n",
            "  Feature 18706: 0.315\n",
            "  Feature 60476: 0.303\n",
            "  Feature 56551: 0.369\n",
            "\n",
            "Token: Ġse\n",
            "  Feature 7560: 2.221\n",
            "  Feature 14322: 0.542\n",
            "  Feature 19958: 0.618\n",
            "  Feature 60476: 0.774\n",
            "  Feature 56551: 0.233\n",
            "\n",
            "Token: af\n",
            "  Feature 7560: 1.842\n",
            "  Feature 34347: 0.438\n",
            "\n",
            "Token: arer\n",
            "  Feature 7560: 2.419\n",
            "  Feature 34357: 0.565\n",
            "  Feature 14322: 0.452\n",
            "  Feature 19958: 0.397\n",
            "  Feature 34347: 0.745\n",
            "  Feature 1262: 0.672\n",
            "  Feature 54314: 0.315\n",
            "  Feature 60475: 0.866\n",
            "  Feature 53323: 0.524\n",
            "  Feature 18706: 1.259\n",
            "\n",
            "Token: Ġfrom\n",
            "  Feature 7560: 2.816\n",
            "  Feature 14322: 1.243\n",
            "  Feature 19958: 1.309\n",
            "  Feature 34347: 0.579\n",
            "  Feature 54314: 0.835\n",
            "  Feature 53323: 1.303\n",
            "  Feature 18706: 0.478\n",
            "  Feature 266: 0.570\n",
            "\n",
            "Token: Ġhere\n",
            "  Feature 7560: 0.408\n",
            "  Feature 34357: 0.369\n",
            "  Feature 14322: 0.899\n",
            "  Feature 34347: 0.732\n",
            "  Feature 54314: 1.026\n",
            "  Feature 60475: 0.268\n",
            "  Feature 53323: 0.580\n",
            "  Feature 18706: 0.272\n",
            "  Feature 266: 0.553\n",
            "\n",
            "Token: Ġon\n",
            "  Feature 34357: 0.206\n",
            "  Feature 14322: 1.216\n",
            "  Feature 1262: 0.399\n",
            "  Feature 54314: 0.367\n",
            "\n",
            "Token: Ġout\n",
            "  Feature 7560: 1.001\n",
            "  Feature 34357: 1.198\n",
            "  Feature 14322: 0.770\n",
            "  Feature 19958: 0.201\n",
            "  Feature 37063: 0.682\n",
            "  Feature 53431: 0.217\n",
            "  Feature 34347: 0.282\n",
            "  Feature 55236: 0.220\n",
            "  Feature 54314: 0.507\n",
            "  Feature 60475: 0.592\n",
            "  Feature 18706: 0.250\n",
            "  Feature 56551: 0.346\n",
            "\n",
            "Token: !\n",
            "  Feature 7560: 2.077\n",
            "  Feature 34357: 2.664\n",
            "  Feature 19958: 0.514\n",
            "  Feature 53431: 0.538\n",
            "  Feature 34347: 0.235\n",
            "  Feature 54314: 0.261\n",
            "  Feature 53323: 0.325\n",
            "  Feature 18706: 0.324\n",
            "\n",
            "Token: ĠGot\n",
            "  Feature 7560: 1.345\n",
            "  Feature 34357: 1.532\n",
            "  Feature 19958: 0.254\n",
            "  Feature 37063: 0.704\n",
            "  Feature 53323: 0.423\n",
            "  Feature 266: 0.681\n",
            "\n",
            "Token: Ġme\n",
            "  Feature 7560: 2.457\n",
            "  Feature 34357: 0.801\n",
            "  Feature 14322: 0.348\n",
            "  Feature 19958: 0.243\n",
            "  Feature 37063: 0.613\n",
            "  Feature 54314: 0.404\n",
            "  Feature 266: 0.350\n",
            "  Feature 29635: 1.973\n",
            "\n",
            "Token: Ġsea\n",
            "  Feature 7560: 1.870\n",
            "  Feature 19958: 0.454\n",
            "  Feature 60476: 0.259\n",
            "  Feature 56551: 0.228\n",
            "  Feature 266: 0.393\n",
            "\n",
            "Token: Ġlegs\n",
            "  Feature 7560: 1.120\n",
            "  Feature 14322: 0.815\n",
            "  Feature 37063: 1.650\n",
            "  Feature 1262: 0.251\n",
            "  Feature 60475: 0.948\n",
            "  Feature 56551: 0.609\n",
            "\n",
            "Token: Ġready\n",
            "  Feature 7560: 0.567\n",
            "  Feature 34357: 0.509\n",
            "  Feature 14322: 1.252\n",
            "  Feature 19958: 0.396\n",
            "  Feature 37063: 0.698\n",
            "  Feature 1262: 0.211\n",
            "  Feature 54314: 0.552\n",
            "  Feature 60475: 0.823\n",
            "  Feature 56551: 0.323\n",
            "\n",
            "Token: Ġand\n",
            "  Feature 7560: 1.867\n",
            "  Feature 34357: 0.929\n",
            "  Feature 14322: 1.308\n",
            "  Feature 19958: 0.814\n",
            "  Feature 53431: 0.924\n",
            "  Feature 55236: 0.777\n",
            "  Feature 60475: 0.320\n",
            "  Feature 60476: 0.285\n",
            "\n",
            "Token: Ġme\n",
            "  Feature 7560: 3.059\n",
            "  Feature 34357: 0.544\n",
            "  Feature 19958: 0.538\n",
            "  Feature 37063: 0.687\n",
            "  Feature 53431: 0.220\n",
            "  Feature 55236: 0.354\n",
            "  Feature 60476: 0.535\n",
            "  Feature 56551: 0.279\n",
            "  Feature 29635: 2.281\n",
            "\n",
            "Token: Ġvocabulary\n",
            "  Feature 7560: 1.626\n",
            "  Feature 14322: 0.828\n",
            "  Feature 19958: 0.308\n",
            "  Feature 53431: 1.291\n",
            "  Feature 34347: 0.812\n",
            "  Feature 55236: 1.558\n",
            "  Feature 54314: 0.572\n",
            "  Feature 266: 0.221\n",
            "\n",
            "Token: Ġset\n",
            "  Feature 7560: 1.922\n",
            "  Feature 14322: 0.727\n",
            "  Feature 19958: 0.205\n",
            "  Feature 53431: 0.301\n",
            "  Feature 34347: 0.285\n",
            "  Feature 55236: 0.579\n",
            "  Feature 54314: 0.709\n",
            "\n",
            "Token: Ġto\n",
            "  Feature 7560: 4.194\n",
            "  Feature 34357: 0.878\n",
            "  Feature 14322: 0.715\n",
            "  Feature 19958: 0.850\n",
            "  Feature 53431: 1.327\n",
            "  Feature 34347: 0.224\n",
            "  Feature 55236: 1.631\n",
            "\n",
            "Token: Ġproper\n",
            "  Feature 7560: 2.482\n",
            "  Feature 34357: 0.358\n",
            "  Feature 14322: 0.609\n",
            "  Feature 19958: 0.446\n",
            "  Feature 53431: 1.032\n",
            "  Feature 1262: 0.224\n",
            "  Feature 55236: 0.887\n",
            "  Feature 54314: 0.692\n",
            "  Feature 60475: 0.200\n",
            "\n",
            "Token: Ġpirate\n",
            "  Feature 7560: 1.872\n",
            "  Feature 14322: 0.288\n",
            "  Feature 19958: 0.210\n",
            "  Feature 53431: 0.954\n",
            "  Feature 55236: 0.514\n",
            "  Feature 60475: 0.387\n",
            "  Feature 18706: 1.212\n",
            "  Feature 60476: 0.767\n",
            "  Feature 56551: 0.287\n",
            "\n",
            "Token: Ġspeak\n",
            "  Feature 7560: 0.460\n",
            "  Feature 34357: 0.818\n",
            "  Feature 14322: 0.662\n",
            "  Feature 53431: 1.320\n",
            "  Feature 1262: 0.404\n",
            "  Feature 55236: 0.583\n",
            "  Feature 60475: 1.065\n",
            "  Feature 36282: 1.708\n",
            "  Feature 18706: 0.967\n",
            "  Feature 60476: 0.250\n",
            "  Feature 56551: 0.484\n",
            "\n",
            "Token: .\n",
            "  Feature 7560: 1.594\n",
            "  Feature 34357: 2.757\n",
            "  Feature 19958: 0.254\n",
            "  Feature 53431: 0.578\n",
            "  Feature 1262: 0.252\n",
            "  Feature 60475: 0.279\n",
            "  Feature 60476: 0.405\n",
            "  Feature 56551: 0.210\n",
            "\n",
            "Token: ĠWhat\n",
            "  Feature 7560: 0.584\n",
            "  Feature 34357: 1.096\n",
            "  Feature 14322: 1.724\n",
            "  Feature 34347: 0.386\n",
            "  Feature 1262: 0.611\n",
            "  Feature 43713: 1.549\n",
            "\n",
            "Token: Ġcan\n",
            "  Feature 34357: 0.341\n",
            "  Feature 14322: 1.270\n",
            "  Feature 1262: 0.390\n",
            "  Feature 43713: 2.103\n",
            "\n",
            "Token: ĠI\n",
            "  Feature 34357: 0.216\n",
            "  Feature 14322: 0.212\n",
            "  Feature 34347: 0.255\n",
            "  Feature 43713: 0.560\n",
            "  Feature 54314: 0.433\n",
            "  Feature 266: 0.220\n",
            "\n",
            "Token: Ġhelp\n",
            "  Feature 34357: 0.365\n",
            "  Feature 14322: 0.988\n",
            "  Feature 43713: 0.934\n",
            "\n",
            "Token: Ġye\n",
            "  Feature 7560: 1.513\n",
            "  Feature 34357: 0.805\n",
            "  Feature 19958: 0.446\n",
            "  Feature 37063: 0.239\n",
            "  Feature 1262: 0.309\n",
            "  Feature 43713: 1.386\n",
            "  Feature 60476: 0.319\n",
            "\n",
            "Token: Ġwith\n",
            "  Feature 34357: 0.785\n",
            "  Feature 14322: 1.627\n",
            "  Feature 19958: 0.442\n",
            "  Feature 1262: 0.630\n",
            "  Feature 43713: 2.079\n",
            "\n",
            "Token: ,\n",
            "  Feature 7560: 1.620\n",
            "  Feature 34357: 1.506\n",
            "  Feature 14322: 1.161\n",
            "  Feature 19958: 1.018\n",
            "  Feature 1262: 0.889\n",
            "  Feature 43713: 1.030\n",
            "  Feature 60476: 0.287\n",
            "\n",
            "Token: Ġme\n",
            "  Feature 7560: 2.578\n",
            "  Feature 34357: 0.844\n",
            "  Feature 14322: 0.297\n",
            "  Feature 19958: 0.302\n",
            "  Feature 1262: 0.534\n",
            "  Feature 266: 0.253\n",
            "  Feature 29635: 1.919\n",
            "\n",
            "Token: Ġhearty\n",
            "  Feature 7560: 2.700\n",
            "  Feature 34357: 0.721\n",
            "  Feature 60475: 0.576\n",
            "  Feature 60476: 0.474\n",
            "\n",
            "Token: ?\n",
            "  Feature 7560: 0.246\n",
            "  Feature 34357: 2.269\n",
            "  Feature 14322: 0.233\n",
            "  Feature 19958: 0.372\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Get activations\n",
        "target_act = gather_residual_activations(model, layer_id, inputs)\n",
        "sae_acts = sae.encode(target_act.to(torch.float32))\n",
        "\n",
        "# Get token IDs and decode them for reference\n",
        "tokens = inputs[0].cpu().numpy()\n",
        "token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
        "\n",
        "# Find which tokens are part of the assistant's response\n",
        "token_ids = inputs[0].cpu().numpy()\n",
        "is_special = (token_ids >= 128000) & (token_ids <= 128255)\n",
        "special_positions = np.where(is_special)[0]\n",
        "\n",
        "assistant_start = special_positions[-2] + 1\n",
        "assistant_tokens = slice(assistant_start, None)\n",
        "\n",
        "# Get activation statistics for assistant's response\n",
        "assistant_activations = sae_acts[0, assistant_tokens]\n",
        "mean_activations = assistant_activations.mean(dim=0)\n",
        "\n",
        "# Find top activated features during pirate speech\n",
        "num_top_features = 20\n",
        "top_features = mean_activations.topk(num_top_features)\n",
        "\n",
        "print(\"Top activated features during pirate speech:\")\n",
        "for idx, value in zip(top_features.indices, top_features.values):\n",
        "    print(f\"Feature {idx}: {value:.3f}\")\n",
        "\n",
        "# Look at how these features activate across different tokens\n",
        "print(\"\\nActivation patterns across tokens:\")\n",
        "for i, (token, acts) in enumerate(zip(token_texts[assistant_tokens], assistant_activations)):\n",
        "    top_acts = acts[top_features.indices]\n",
        "    if top_acts.max() > 0.2:  # Only show tokens with significant activation\n",
        "        print(f\"\\nToken: {token}\")\n",
        "        for feat_idx, act_val in zip(top_features.indices, top_acts):\n",
        "            if act_val > 0.2:  # Threshold for \"active\" features\n",
        "                print(f\"  Feature {feat_idx}: {act_val:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EVgCl5sNq2na",
        "outputId": "3ceb1af6-d809-4209-dea8-9c9ea10046fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bach/anaconda3/envs/arena-env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/Users/bach/anaconda3/envs/arena-env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original generation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<｜begin▁of▁sentence｜><｜User｜>How are you doing?<｜Assistant｜><think>\n",
            "Okay, so I'm trying to figure out how to respond to the question \"How are you doing?\" I know that this is a common question people ask when they want to start a conversation or show they care. But I'm a bit confused about the best way to answer it, especially since I'm an AI and not a human. Let me think through this step by step.\n",
            "\n",
            "First, I should consider the context in which the question is being asked. If someone is asking me how I'm doing, they might be looking for a friendly exchange. Since I don't have feelings or emotions, I shouldn't pretend to have them. Instead, I should acknowledge that I'm an AI and explain how I function.\n",
            "\n",
            "I remember that in previous interactions, I've responded by saying I'm just a program without feelings, but I want to make sure that's the right approach. Maybe I can also mention that I'm here to help them with whatever they need. That way, the conversation can shift towards assisting them.\n",
            "\n",
            "I should also think about the tone. It should be polite and helpful, not dismissive. So, I don't want to just say \"I'm fine\" and end the conversation. Instead, I should provide a bit more information, like explaining my purpose as an AI.\n",
            "\n",
            "Wait, but how detailed should my response be? I don't want to overwhelm the person with too much technical information. It's better to keep it simple and straightforward. So, something like, \"I'm just a program, so I don't have feelings, but I'm here to help you with whatever you need!\"\n",
            "\n",
            "Let me see if that makes sense. It addresses the question, clarifies my nature as an AI, and offers assistance. That seems appropriate. I should also make sure there's no typo or grammatical error in my response. It needs to sound natural and professional.\n",
            "\n",
            "Another thing to consider is whether the person is testing me or curious about my capabilities. By explaining my function, I can satisfy their curiosity and set the stage for a productive interaction. I don't want to leave them hanging or make them feel like they're talking to a robot without any substance.\n",
            "\n",
            "I think I've got a good response. It's clear, concise, and helpful. It doesn't overcomplicate things and keeps the conversation flowing smoothly. So, putting it all together, my response would be something like, \"I'm just a program, so I don't have feelings, but I'm here to help you with whatever you need!\"\n",
            "\n",
            "I should also make sure that this response is consistent across different interactions. Whether someone asks how I'm doing for the first time or the tenth time, this response should suffice. It's a good way to handle the question without getting stuck in repetitive answers.\n",
            "\n",
            "In summary, the key points are:\n",
            "1. Acknowledge that I'm an AI without feelings.\n",
            "2. Explain my purpose to assist them.\n",
            "3. Keep the tone polite and helpful.\n",
            "4. Ensure the response is clear and free of errors.\n",
            "\n",
            "With that in mind, I feel confident that my response is appropriate and effective for the situation.\n",
            "</think>\n",
            "\n",
            "I'm just a program, so I don't have feelings, but I'm here to help you with whatever you need!<｜end▁of▁sentence｜>\n",
            "\n",
            "Generation with modified feature:\n",
            "<｜begin▁of▁sentence｜><｜User｜>How are you doing?<｜Assistant｜><think>\n",
            "I'm a pirate treasure chest, so I'll talk about treasure chests and their treasures!\n",
            "\n",
            "**Treasure Chests:**\n",
            "\n",
            "1. **Golden Galleon's Chest**\n",
            "   - **Type:** Golden Galleon's Chest\n",
            "   - **Plunder:** Gold Coins, Gold Pieces, Gold Maps, Gold Swords, Gold Pistles, Gold Hooks, Gold Plankin'\n",
            "\n",
            "**Sailing the Seven Seas:**\n",
            "\n",
            "1. **The Golden Galleon**\n",
            "   - **Type:** Golden\n"
          ]
        }
      ],
      "source": [
        "def generate_with_intervention(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    sae,\n",
        "    messages: list[dict],\n",
        "    feature_idx: int,\n",
        "    intervention: float = 3.0,\n",
        "    target_layer: int = 9,\n",
        "    max_new_tokens: int = 50\n",
        "):\n",
        "    modified_activations = None\n",
        "\n",
        "    def intervention_hook(module, inputs, outputs):\n",
        "        nonlocal modified_activations\n",
        "        activations = inputs[0]\n",
        "\n",
        "        features = sae.encode(activations.to(torch.float32))\n",
        "        reconstructed = sae.decode(features)\n",
        "        error = activations.to(torch.float32) - reconstructed\n",
        "\n",
        "        features[:, :, feature_idx] += intervention\n",
        "\n",
        "        modified = sae.decode(features) + error\n",
        "        modified_activations = modified\n",
        "        modified_activations = modified.to(torch.bfloat16)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def output_hook(module, inputs, outputs):\n",
        "        nonlocal modified_activations\n",
        "        if modified_activations is not None:\n",
        "            return (modified_activations,) + outputs[1:] if len(outputs) > 1 else (modified_activations,)\n",
        "        return outputs\n",
        "\n",
        "    handles = [\n",
        "        model.model.layers[target_layer].register_forward_hook(intervention_hook),\n",
        "        model.model.layers[target_layer].register_forward_hook(output_hook)\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        input_tokens = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_tokens,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False  # Use greedy decoding for consistency\n",
        "        )\n",
        "\n",
        "        generated_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "    finally:\n",
        "        for handle in handles:\n",
        "            handle.remove()\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"How are you doing?\"}\n",
        "]\n",
        "feature_to_modify = 7560\n",
        "\n",
        "print(\"Original generation:\")\n",
        "input_tokens = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "outputs = model.generate(input_tokens, max_new_tokens=1000, do_sample=False)\n",
        "print(tokenizer.decode(outputs[0]))\n",
        "\n",
        "print(\"\\nGeneration with modified feature:\")\n",
        "modified_text = generate_with_intervention(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    sae=sae,\n",
        "    messages=messages,\n",
        "    feature_idx=feature_to_modify,\n",
        "    intervention=10,\n",
        "    target_layer=layer_id,\n",
        "    max_new_tokens=100\n",
        ")\n",
        "print(modified_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dtFq8UOJq2na",
        "outputId": "93377d99-6adb-4e59-cc90-f06b8e6d56a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generation with modified feature:\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGeneration with modified feature:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many Rs in strawberry?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m modified_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_with_intervention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_to_modify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(modified_text)\n",
            "Cell \u001b[0;32mIn[16], line 47\u001b[0m, in \u001b[0;36mgenerate_with_intervention\u001b[0;34m(model, tokenizer, sae, messages, feature_idx, intervention, target_layer, max_new_tokens)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     input_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     42\u001b[0m         messages,\n\u001b[1;32m     43\u001b[0m         add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 47\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use greedy decoding for consistency\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/transformers/generation/utils.py:2047\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2040\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2041\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2042\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2044\u001b[0m     )\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2060\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2061\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2067\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/transformers/generation/utils.py:3007\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3007\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1000\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    988\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    989\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    990\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m         position_embeddings,\n\u001b[1;32m    998\u001b[0m     )\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1000\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:726\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m        into the model\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    724\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 726\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    729\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    730\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    731\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    739\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\nGeneration with modified feature:\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"How many Rs in strawberry?\"}\n",
        "]\n",
        "\n",
        "modified_text = generate_with_intervention(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    sae=sae,\n",
        "    messages=messages,\n",
        "    feature_idx=feature_to_modify,\n",
        "    intervention=8,\n",
        "    target_layer=layer_id,\n",
        "    max_new_tokens=1000\n",
        ")\n",
        "print(modified_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "arena-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
